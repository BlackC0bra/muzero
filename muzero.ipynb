{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "muzero",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bvBjwVcXYgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lint as: python3\n",
        "\"\"\"Pseudocode description of the MuZero algorithm.\"\"\"\n",
        "# pylint: disable=unused-argument\n",
        "# pylint: disable=missing-docstring\n",
        "# pylint: disable=assignment-from-no-return\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import math\n",
        "import typing\n",
        "from typing import Dict, List, Optional\n",
        "import enum\n",
        "\n",
        "import numpy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "import threading\n",
        "\n",
        "##########################\n",
        "####### Helpers ##########\n",
        "\n",
        "MAXIMUM_FLOAT_VALUE = float('inf')\n",
        "\n",
        "KnownBounds = collections.namedtuple('KnownBounds', ['min', 'max'])\n",
        "\n",
        "# noinspection PyArgumentList\n",
        "Winner = enum.Enum(\"Winner\", \"black white draw\")\n",
        "\n",
        "# noinspection PyArgumentList\n",
        "Player = enum.Enum(\"Player\", \"black white\")\n",
        "\n",
        "num_filters = 2\n",
        "num_blocks = 1\n",
        "\n",
        "class MinMaxStats(object):\n",
        "  \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
        "\n",
        "  def __init__(self, known_bounds: Optional[KnownBounds]):\n",
        "    self.maximum = known_bounds.max if known_bounds else -MAXIMUM_FLOAT_VALUE\n",
        "    self.minimum = known_bounds.min if known_bounds else MAXIMUM_FLOAT_VALUE\n",
        "\n",
        "  def update(self, value: float):\n",
        "    self.maximum = max(self.maximum, value)\n",
        "    self.minimum = min(self.minimum, value)\n",
        "\n",
        "  def normalize(self, value: float) -> float:\n",
        "    if self.maximum > self.minimum:\n",
        "      # We normalize only when we have set the maximum and minimum values.\n",
        "      return (value - self.minimum) / (self.maximum - self.minimum)\n",
        "    return value\n",
        "\n",
        "\n",
        "class MuZeroConfig(object):\n",
        "\n",
        "  def __init__(self,\n",
        "               action_space_size: int,\n",
        "               max_moves: int,\n",
        "               discount: float,\n",
        "               dirichlet_alpha: float,\n",
        "               num_simulations: int,\n",
        "               batch_size: int,\n",
        "               td_steps: int,\n",
        "               num_actors: int,\n",
        "               lr_init: float,\n",
        "               lr_decay_steps: float,\n",
        "               visit_softmax_temperature_fn,\n",
        "               known_bounds: Optional[KnownBounds] = None):\n",
        "    ### Self-Play\n",
        "    self.action_space_size = action_space_size\n",
        "    self.num_actors = num_actors\n",
        "\n",
        "    self.visit_softmax_temperature_fn = visit_softmax_temperature_fn\n",
        "    self.max_moves = max_moves\n",
        "    self.num_simulations = num_simulations\n",
        "    self.discount = discount\n",
        "\n",
        "    # Root prior exploration noise.\n",
        "    self.root_dirichlet_alpha = dirichlet_alpha\n",
        "    self.root_exploration_fraction = 0.25\n",
        "\n",
        "    # UCB formula\n",
        "    self.pb_c_base = 19652\n",
        "    self.pb_c_init = 1.25\n",
        "\n",
        "    # If we already have some information about which values occur in the\n",
        "    # environment, we can use them to initialize the rescaling.\n",
        "    # This is not strictly necessary, but establishes identical behaviour to\n",
        "    # AlphaZero in board games.\n",
        "    self.known_bounds = known_bounds\n",
        "\n",
        "    ### Training\n",
        "    self.training_steps = int(5)\n",
        "    self.checkpoint_interval = int(1e3)\n",
        "    self.window_size = int(1e6)\n",
        "    self.batch_size = batch_size\n",
        "    self.num_unroll_steps = 3\n",
        "    self.td_steps = td_steps\n",
        "\n",
        "    self.weight_decay = 1e-4\n",
        "    self.momentum = 0.9\n",
        "\n",
        "    # Exponential learning rate schedule\n",
        "    self.lr_init = lr_init\n",
        "    self.lr_decay_rate = 0.1\n",
        "    self.lr_decay_steps = lr_decay_steps\n",
        "\n",
        "  def new_game(self):\n",
        "    return Game(self.action_space_size, self.discount)\n",
        "\n",
        "\n",
        "def make_board_game_config(action_space_size: int, max_moves: int,\n",
        "                           dirichlet_alpha: float,\n",
        "                           lr_init: float) -> MuZeroConfig:\n",
        "\n",
        "  def visit_softmax_temperature(num_moves, training_steps):\n",
        "    if num_moves < 30:\n",
        "      return 1.0\n",
        "    else:\n",
        "      return 0.0  # Play according to the max.\n",
        "\n",
        "  return MuZeroConfig(\n",
        "      action_space_size=action_space_size,\n",
        "      max_moves=max_moves,\n",
        "      discount=1.0,\n",
        "      dirichlet_alpha=dirichlet_alpha,\n",
        "      num_simulations=5,\n",
        "      batch_size=64,\n",
        "      td_steps=max_moves,  # Always use Monte Carlo return.\n",
        "      num_actors=2,\n",
        "      lr_init=lr_init,\n",
        "      lr_decay_steps=400e3,\n",
        "      visit_softmax_temperature_fn=visit_softmax_temperature,\n",
        "      known_bounds=KnownBounds(-1, 1))\n",
        "\n",
        "def make_connect4_config() -> MuZeroConfig:\n",
        "  return make_board_game_config(\n",
        "      action_space_size=7, max_moves=42, dirichlet_alpha=0.03, lr_init=0.01)\n",
        "\n",
        "class Action(object):\n",
        "\n",
        "  def __init__(self, index: int):\n",
        "    self.index = index\n",
        "\n",
        "  def __hash__(self):\n",
        "    return self.index\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    return self.index == other\n",
        "\n",
        "  def __gt__(self, other):\n",
        "    return self.index > other\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vGSYHVMX-T_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Node(object):\n",
        "\n",
        "  def __init__(self, prior: float):\n",
        "    self.visit_count = 0\n",
        "    self.to_play = -1\n",
        "    self.prior = prior\n",
        "    self.value_sum = 0\n",
        "    self.children = {}\n",
        "    self.hidden_state = None\n",
        "    self.reward = 0\n",
        "\n",
        "  def expanded(self) -> bool:\n",
        "    return len(self.children) > 0\n",
        "\n",
        "  def value(self) -> float:\n",
        "    if self.visit_count == 0:\n",
        "      return 0\n",
        "    return self.value_sum / self.visit_count\n",
        "\n",
        "\n",
        "class ActionHistory(object):\n",
        "  \"\"\"Simple history container used inside the search.\n",
        "\n",
        "  Only used to keep track of the actions executed.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, history: List[Action], action_space_size: int):\n",
        "    self.history = list(history)\n",
        "    self.action_space_size = action_space_size\n",
        "\n",
        "  def clone(self):\n",
        "    return ActionHistory(self.history, self.action_space_size)\n",
        "\n",
        "  def add_action(self, action: Action):\n",
        "    self.history.append(action)\n",
        "\n",
        "  def last_action(self) -> Action:\n",
        "    return self.history[-1]\n",
        "\n",
        "  def action_space(self) -> List[Action]:\n",
        "    return [i for i in range(self.action_space_size)]\n",
        "\n",
        "  def to_play(self) -> Player:\n",
        "    if len(self.history) % 2 == 0:\n",
        "      return Player.white\n",
        "    else:\n",
        "      return Player.black\n",
        "\n",
        "class Environment(object):\n",
        "  \"\"\"The environment MuZero is interacting with.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "      self.board = None\n",
        "      self.turn = 0\n",
        "      self.done = False\n",
        "      self.winner = None  # type: Winner\n",
        "      self.resigned = False\n",
        "\n",
        "  def reset(self):\n",
        "      self.board = []\n",
        "      for i in range(6):\n",
        "          self.board.append([])\n",
        "          for j in range(7): # pylint: disable=unused-variable\n",
        "              self.board[i].append(' ')\n",
        "      self.turn = 0\n",
        "      self.done = False\n",
        "      self.winner = None\n",
        "      self.resigned = False\n",
        "      return self\n",
        "\n",
        "  def update(self, board):\n",
        "      self.board = numpy.copy(board)\n",
        "      self.turn = self.turn_n()\n",
        "      self.done = False\n",
        "      self.winner = None\n",
        "      self.resigned = False\n",
        "      return self\n",
        "\n",
        "  def turn_n(self):\n",
        "      turn = 0\n",
        "      for i in range(6):\n",
        "          for j in range(7):\n",
        "              if self.board[i][j] != ' ':\n",
        "                  turn += 1\n",
        "\n",
        "      return turn\n",
        "\n",
        "  def player_turn(self):\n",
        "      if self.turn % 2 == 0:\n",
        "          return Player.white\n",
        "      else:\n",
        "          return Player.black\n",
        "\n",
        "  def step(self, action):\n",
        "      for i in range(6):\n",
        "          if self.board[i][action] == ' ':\n",
        "              self.board[i][action] = ('X' if self.player_turn() == Player.white else 'O')\n",
        "              break\n",
        "\n",
        "      self.turn += 1\n",
        "\n",
        "      self.check_for_fours()\n",
        "\n",
        "      if self.turn >= 42:\n",
        "          self.done = True\n",
        "          if self.winner is None:\n",
        "              self.winner = Winner.draw\n",
        "\n",
        "      r = 0\n",
        "      if self.done:\n",
        "        if self.turn % 2 == 0:\n",
        "          if Winner.white:\n",
        "            r = 1\n",
        "          elif Winner.black:\n",
        "            r = -1\n",
        "        else:\n",
        "          if Winner.black:\n",
        "            r = 1\n",
        "          elif Winner.white:\n",
        "            r = -1\n",
        "\n",
        "      return r\n",
        "\n",
        "  def legal_moves(self):\n",
        "      legal = [0, 0, 0, 0, 0, 0, 0]\n",
        "      for j in range(7):\n",
        "          for i in range(6):\n",
        "              if self.board[i][j] == ' ':\n",
        "                  legal[j] = 1\n",
        "                  break\n",
        "      return legal\n",
        "\n",
        "  def legal_actions(self):\n",
        "      legal = []\n",
        "      for j in range(7):\n",
        "          for i in range(6):\n",
        "              if self.board[i][j] == ' ':\n",
        "                  legal.append(j)\n",
        "                  break\n",
        "      return legal\n",
        "\n",
        "  def check_for_fours(self):\n",
        "      for i in range(6):\n",
        "          for j in range(7):\n",
        "              if self.board[i][j] != ' ':\n",
        "                  # check if a vertical four-in-a-row starts at (i, j)\n",
        "                  if self.vertical_check(i, j):\n",
        "                      self.done = True\n",
        "                      return\n",
        "\n",
        "                  # check if a horizontal four-in-a-row starts at (i, j)\n",
        "                  if self.horizontal_check(i, j):\n",
        "                      self.done = True\n",
        "                      return\n",
        "\n",
        "                  # check if a diagonal (either way) four-in-a-row starts at (i, j)\n",
        "                  diag_fours = self.diagonal_check(i, j)\n",
        "                  if diag_fours:\n",
        "                      self.done = True\n",
        "                      return\n",
        "\n",
        "  def vertical_check(self, row, col):\n",
        "      # print(\"checking vert\")\n",
        "      four_in_a_row = False\n",
        "      consecutive_count = 0\n",
        "\n",
        "      for i in range(row, 6):\n",
        "          if self.board[i][col].lower() == self.board[row][col].lower():\n",
        "              consecutive_count += 1\n",
        "          else:\n",
        "              break\n",
        "\n",
        "      if consecutive_count >= 4:\n",
        "          four_in_a_row = True\n",
        "          if 'x' == self.board[row][col].lower():\n",
        "              self.winner = Winner.white\n",
        "          else:\n",
        "              self.winner = Winner.black\n",
        "\n",
        "      return four_in_a_row\n",
        "\n",
        "  def horizontal_check(self, row, col):\n",
        "      four_in_a_row = False\n",
        "      consecutive_count = 0\n",
        "\n",
        "      for j in range(col, 7):\n",
        "          if self.board[row][j].lower() == self.board[row][col].lower():\n",
        "              consecutive_count += 1\n",
        "          else:\n",
        "              break\n",
        "\n",
        "      if consecutive_count >= 4:\n",
        "          four_in_a_row = True\n",
        "          if 'x' == self.board[row][col].lower():\n",
        "              self.winner = Winner.white\n",
        "          else:\n",
        "              self.winner = Winner.black\n",
        "\n",
        "      return four_in_a_row\n",
        "\n",
        "  def diagonal_check(self, row, col):\n",
        "      four_in_a_row = False\n",
        "      count = 0\n",
        "\n",
        "      consecutive_count = 0\n",
        "      j = col\n",
        "      for i in range(row, 6):\n",
        "          if j > 6:\n",
        "              break\n",
        "          elif self.board[i][j].lower() == self.board[row][col].lower():\n",
        "              consecutive_count += 1\n",
        "          else:\n",
        "              break\n",
        "          j += 1\n",
        "\n",
        "      if consecutive_count >= 4:\n",
        "          count += 1\n",
        "          if 'x' == self.board[row][col].lower():\n",
        "              self.winner = Winner.white\n",
        "          else:\n",
        "              self.winner = Winner.black\n",
        "\n",
        "      consecutive_count = 0\n",
        "      j = col\n",
        "      for i in range(row, -1, -1):\n",
        "          if j > 6:\n",
        "              break\n",
        "          elif self.board[i][j].lower() == self.board[row][col].lower():\n",
        "              consecutive_count += 1\n",
        "          else:\n",
        "              break\n",
        "          j += 1\n",
        "\n",
        "      if consecutive_count >= 4:\n",
        "          count += 1\n",
        "          if 'x' == self.board[row][col].lower():\n",
        "              self.winner = Winner.white\n",
        "          else:\n",
        "              self.winner = Winner.black\n",
        "\n",
        "      if count > 0:\n",
        "          four_in_a_row = True\n",
        "\n",
        "      return four_in_a_row\n",
        "\n",
        "  def black_and_white_plane(self):\n",
        "      board_white = numpy.copy(self.board)\n",
        "      board_black = numpy.copy(self.board)\n",
        "      for i in range(6):\n",
        "          for j in range(7):\n",
        "              if self.board[i][j] == ' ':\n",
        "                  board_white[i][j] = 0\n",
        "                  board_black[i][j] = 0\n",
        "              elif self.board[i][j] == 'X':\n",
        "                  board_white[i][j] = 1\n",
        "                  board_black[i][j] = 0\n",
        "              else:\n",
        "                  board_white[i][j] = 0\n",
        "                  board_black[i][j] = 1\n",
        "\n",
        "      return numpy.array(board_white), numpy.array(board_black)\n",
        "\n",
        "  def render(self):\n",
        "      print(\"\\nRound: \" + str(self.turn))\n",
        "\n",
        "      for i in range(5, -1, -1):\n",
        "          print(\"\\t\", end=\"\")\n",
        "          for j in range(7):\n",
        "              print(\"| \" + str(self.board[i][j]), end=\" \")\n",
        "          print(\"|\")\n",
        "      print(\"\\t  _   _   _   _   _   _   _ \")\n",
        "      print(\"\\t  1   2   3   4   5   6   7 \")\n",
        "\n",
        "      if self.done:\n",
        "          print(\"Game Over!\")\n",
        "          if self.winner == Winner.white:\n",
        "              print(\"X is the winner\")\n",
        "          elif self.winner == Winner.black:\n",
        "              print(\"O is the winner\")\n",
        "          else:\n",
        "              print(\"Game was a draw\")\n",
        "\n",
        "  @property\n",
        "  def observation(self):\n",
        "      return ''.join(''.join(x for x in y) for y in self.board)\n",
        "\n",
        "\n",
        "class Game(object):\n",
        "  \"\"\"A single episode of interaction with the environment.\"\"\"\n",
        "\n",
        "  def __init__(self, action_space_size: int, discount: float):\n",
        "    self.environment = Environment().reset()  # Game specific environment.\n",
        "    self.history = []\n",
        "    self.rewards = []\n",
        "    self.child_visits = []\n",
        "    self.root_values = []\n",
        "    self.action_space_size = action_space_size\n",
        "    self.discount = discount\n",
        "\n",
        "  def terminal(self) -> bool:\n",
        "    # Game specific termination rules.\n",
        "    return self.environment.done\n",
        "\n",
        "  def legal_actions(self) -> List[Action]:\n",
        "    # Game specific calculation of legal actions.\n",
        "    return self.environment.legal_actions()\n",
        "\n",
        "  def apply(self, action: Action):\n",
        "    reward = self.environment.step(action)\n",
        "    reward = reward if self.environment.turn % 2 != 0 and reward == 1 else -reward\n",
        "    self.rewards.append(reward)\n",
        "    self.history.append(action)\n",
        "\n",
        "  def store_search_statistics(self, root: Node):\n",
        "    sum_visits = sum(child.visit_count for child in root.children.values())\n",
        "    action_space = (Action(index) for index in range(self.action_space_size))\n",
        "    self.child_visits.append([\n",
        "        root.children[a].visit_count / sum_visits if a in root.children else 0\n",
        "        for a in action_space\n",
        "    ])\n",
        "    self.root_values.append(root.value())\n",
        "\n",
        "  def make_image(self, state_index: int):\n",
        "    # Game specific feature planes.    \n",
        "    o = Environment().reset()\n",
        "\n",
        "    for current_index in range(0, state_index):\n",
        "      o.step(self.history[current_index])\n",
        "\n",
        "    black_ary, white_ary = o.black_and_white_plane()\n",
        "    state = [black_ary, white_ary] if o.player_turn() == Player.black else [white_ary, black_ary]\n",
        "    return numpy.array(state)\n",
        "\n",
        "  def make_target(self, state_index: int, num_unroll_steps: int, td_steps: int,\n",
        "                  to_play: Player):\n",
        "    # The value target is the discounted root value of the search tree N steps\n",
        "    # into the future, plus the discounted sum of all rewards until then.\n",
        "    targets = []\n",
        "    for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
        "      bootstrap_index = current_index + td_steps\n",
        "      if bootstrap_index < len(self.root_values):\n",
        "        value = self.root_values[bootstrap_index] * self.discount**td_steps\n",
        "      else:\n",
        "        value = 0\n",
        "\n",
        "      for i, reward in enumerate(self.rewards[current_index:bootstrap_index]):\n",
        "        value += reward * self.discount**i  # pytype: disable=unsupported-operands\n",
        "\n",
        "      if current_index < len(self.root_values):\n",
        "        targets.append((value, self.rewards[current_index],\n",
        "                        self.child_visits[current_index]))\n",
        "      else:\n",
        "        # States past the end of games are treated as absorbing states.\n",
        "        targets.append((0, 0, []))\n",
        "    return targets\n",
        "\n",
        "  def to_play(self) -> Player:\n",
        "    return self.environment.player_turn\n",
        "\n",
        "  def action_history(self) -> ActionHistory:\n",
        "    return ActionHistory(self.history, self.action_space_size)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-gvlZwpYGVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, config: MuZeroConfig):\n",
        "    self.window_size = config.window_size\n",
        "    self.batch_size = config.batch_size\n",
        "    self.buffer = []\n",
        "\n",
        "  def save_game(self, game):\n",
        "    if len(self.buffer) > self.window_size:\n",
        "      self.buffer.pop(0)\n",
        "    self.buffer.append(game)\n",
        "\n",
        "  def sample_batch(self, num_unroll_steps: int, td_steps: int):\n",
        "    games = [self.sample_game() for _ in range(self.batch_size)]\n",
        "    game_pos = [(g, self.sample_position(g)) for g in games]\n",
        "    return [(g.make_image(i), g.history[i:i + num_unroll_steps],\n",
        "             g.make_target(i, num_unroll_steps, td_steps, g.to_play()))\n",
        "            for (g, i) in game_pos]\n",
        "\n",
        "  def sample_game(self) -> Game:\n",
        "    # Sample game from buffer either uniformly or according to some priority.\n",
        "    return numpy.random.choice(self.buffer)\n",
        "\n",
        "  def sample_position(self, game) -> int:\n",
        "    # Sample position from game either uniformly or according to some priority.\n",
        "    return numpy.random.choice(game.history)\n",
        "\n",
        "# Nets\n",
        "class NetworkOutput(typing.NamedTuple):\n",
        "  value: float\n",
        "  reward: float\n",
        "  policy_logits: Dict[Action, float]\n",
        "  hidden_state: List[float]\n",
        "\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, filters0, filters1, kernel_size, bn=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(filters0, filters1, kernel_size, stride=1, padding=kernel_size//2, bias=False)\n",
        "        self.bn = None\n",
        "        if bn:\n",
        "            self.bn = nn.BatchNorm2d(filters1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            h = self.bn(h)\n",
        "        return h\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, filters):\n",
        "        super().__init__()\n",
        "        self.conv = Conv(filters, filters, 3, True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu(x + (self.conv(x)))\n",
        "\n",
        "class Representation(nn.Module):\n",
        "    ''' Conversion from observation to inner abstract state '''\n",
        "    def __init__(self, input_shape):\n",
        "        super().__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.board_size = self.input_shape[1] * self.input_shape[2]\n",
        "\n",
        "        self.layer0 = Conv(self.input_shape[0], num_filters, 3, bn=True)\n",
        "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.layer0(x))\n",
        "        for block in self.blocks:\n",
        "            h = block(h)\n",
        "        return h\n",
        "\n",
        "class Prediction(nn.Module):\n",
        "    ''' Policy and value prediction from inner abstract state '''\n",
        "    def __init__(self, action_shape):\n",
        "        super().__init__()\n",
        "        self.board_size = 42\n",
        "        self.action_size = action_shape\n",
        "\n",
        "        self.conv_p1 = Conv(num_filters, 4, 1, bn=True)\n",
        "        self.conv_p2 = Conv(4, 1, 1)\n",
        "\n",
        "        self.conv_v = Conv(num_filters, 4, 1, bn=True)\n",
        "        self.fc_v = nn.Linear(self.board_size * 4, 1, bias=False)\n",
        "\n",
        "    def forward(self, rp):\n",
        "        h_p = F.relu(self.conv_p1(rp))\n",
        "        h_p = self.conv_p2(h_p).view(-1, self.action_size)\n",
        "\n",
        "        h_v = F.relu(self.conv_v(rp))\n",
        "        h_v = self.fc_v(h_v.view(-1, self.board_size * 4))\n",
        "\n",
        "        # range of value is -1 ~ 1\n",
        "        return F.softmax(h_p, dim=-1), torch.tanh(h_v)\n",
        "\n",
        "class Dynamics(nn.Module):\n",
        "    '''Abstruct state transition'''\n",
        "    def __init__(self, rp_shape, act_shape):\n",
        "        super().__init__()\n",
        "        self.rp_shape = rp_shape\n",
        "        self.layer0 = Conv(rp_shape[0] + act_shape[0], num_filters, 3, bn=True)\n",
        "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, rp, a):\n",
        "        h = torch.cat([rp, a], dim=1)\n",
        "        h = self.layer0(h)\n",
        "        for block in self.blocks:\n",
        "            h = block(h)\n",
        "        return h\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "  def __init__(self, action_space_size: int):\n",
        "    super().__init__()\n",
        "    self.steps = 0\n",
        "    self.action_space_size = action_space_size\n",
        "    input_shape = (2, 6, 7)\n",
        "    rp_shape = (num_filters, *input_shape[1:])\n",
        "    self.representation = Representation(input_shape).to(device)\n",
        "    self.prediction = Prediction(action_space_size).to(device)\n",
        "    self.dynamics = Dynamics(rp_shape, (2, 6, 7)).to(device)\n",
        "    self.eval()\n",
        "  \n",
        "  def predict_initial_inference(self, x):    \n",
        "    assert x.ndim in (3, 4)\n",
        "    assert x.shape == (2, 6, 7) or x.shape[1:] == (2, 6, 7)\n",
        "    orig_x = x\n",
        "    if x.ndim == 3:\n",
        "        x = x.reshape(1, 2, 6, 7)\n",
        "    \n",
        "    x = torch.Tensor(x).to(device)\n",
        "    h = self.representation(x)\n",
        "    policy, value = self.prediction(h)\n",
        "    \n",
        "    if orig_x.ndim == 3:\n",
        "        return h[0], policy[0], value[0]\n",
        "    else:\n",
        "        return h, policy, value\n",
        "\n",
        "  def predict_recurrent_inference(self, x, a):\n",
        "\n",
        "    if x.ndim == 3:\n",
        "      x = x.reshape(1, 2, 6, 7)\n",
        "\n",
        "    a = numpy.full((1, 2, 6, 7), a)\n",
        "\n",
        "    g = self.dynamics(x, torch.Tensor(a).to(device))\n",
        "    policy, value = self.prediction(g)\n",
        "    \n",
        "    return g[0], policy[0], value[0]\n",
        "\n",
        "  def initial_inference(self, image) -> NetworkOutput:\n",
        "    # representation + prediction function\n",
        "    h, p, v = self.predict_initial_inference(image.astype(numpy.float32))\n",
        "    return NetworkOutput(v, 0, p, h)\n",
        "\n",
        "  def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\n",
        "    # dynamics + prediction function\n",
        "    g, p, v = self.predict_recurrent_inference(hidden_state, action)\n",
        "    return NetworkOutput(v, 0, p, g) \n",
        "\n",
        "  def training_steps(self) -> int:\n",
        "    # How many steps / batches the network has been trained for.\n",
        "    return self.steps\n",
        "\n",
        "\n",
        "class SharedStorage(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._networks = {}\n",
        "\n",
        "  def latest_network(self) -> Network:\n",
        "    if self._networks:\n",
        "      return self._networks[max(self._networks.keys())]\n",
        "    else:\n",
        "      # policy -> uniform, value -> 0, reward -> 0\n",
        "      return make_uniform_network()\n",
        "\n",
        "  def save_network(self, step: int, network: Network):\n",
        "    self._networks[step] = network\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50NBNHaqYPP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "################################################################################\n",
        "############################# Testing the latest net ###########################\n",
        "################################################################################\n",
        "\n",
        "# Battle against random agents\n",
        "def vs_random(network, n=100):\n",
        "    results = {}\n",
        "    for i in range(n):\n",
        "        first_turn = i % 2 == 0\n",
        "        turn = first_turn\n",
        "        game = config.new_game()\n",
        "        r = 0\n",
        "        while not game.terminal():\n",
        "            if turn:\n",
        "              root = Node(0)\n",
        "              current_observation = game.make_image(-1)\n",
        "              expand_node(root, game.to_play(), game.legal_actions(),\n",
        "                          network.initial_inference(current_observation))\n",
        "              add_exploration_noise(config, root)\n",
        "              run_mcts(config, root, game.action_history(), network)\n",
        "              action = select_action(config, len(game.history), root, network)\n",
        "            else:\n",
        "              action = numpy.random.choice(game.legal_actions())\n",
        "            game.apply(action)\n",
        "            turn = not turn\n",
        "        if ((game.environment.winner == Winner.white and first_turn) \n",
        "            or (game.environment.winner == Winner.black and not first_turn)):\n",
        "          r = 1\n",
        "        elif ((game.environment.winner == Winner.black and first_turn) \n",
        "            or (game.environment.winner == Winner.white and not first_turn)):\n",
        "          r = -1\n",
        "        results[r] = results.get(r, 0) + 1\n",
        "    return results\n",
        "\n",
        "def random_vs_random(n=100):\n",
        "    results = {}\n",
        "    for i in range(n):\n",
        "        first_turn = i % 2 == 0\n",
        "        turn = first_turn\n",
        "        game = config.new_game()\n",
        "        r = 0\n",
        "        while not game.terminal():\n",
        "            action = numpy.random.choice(game.legal_actions())\n",
        "            game.apply(action)\n",
        "            turn = not turn\n",
        "        if ((game.environment.winner == Winner.white and first_turn) \n",
        "            or (game.environment.winner == Winner.black and not first_turn)):\n",
        "          r = 1\n",
        "        elif ((game.environment.winner == Winner.black and first_turn) \n",
        "            or (game.environment.winner == Winner.white and not first_turn)):\n",
        "          r = -1\n",
        "        results[r] = results.get(r, 0) + 1\n",
        "    return results\n",
        "\n",
        "##### End Helpers ########\n",
        "##########################\n",
        "\n",
        "\n",
        "# MuZero training is split into two independent parts: Network training and\n",
        "# self-play data generation.\n",
        "# These two parts only communicate by transferring the latest network checkpoint\n",
        "# from the training to the self-play, and the finished games from the self-play\n",
        "# to the training.\n",
        "def muzero(config: MuZeroConfig):\n",
        "  storage = SharedStorage()\n",
        "  replay_buffer = ReplayBuffer(config)\n",
        "\n",
        "  # Start n concurrent actor threads\n",
        "  threads = list()\n",
        "  for _ in range(config.num_actors):\n",
        "    t = threading.Thread(target=launch_job, args=(run_selfplay, config, storage, replay_buffer))\n",
        "    threads.append(t)\n",
        "\n",
        "  # Start all threads\n",
        "  for x in threads:\n",
        "    x.start() \n",
        "\n",
        "  train_network(config, storage, replay_buffer)\n",
        "\n",
        "  return storage.latest_network()\n",
        "\n",
        "\n",
        "##################################\n",
        "####### Part 1: Self-Play ########\n",
        "\n",
        "\n",
        "# Each self-play job is independent of all others; it takes the latest network\n",
        "# snapshot, produces a game and makes it available to the training job by\n",
        "# writing it to a shared replay buffer.\n",
        "def run_selfplay(config: MuZeroConfig, storage: SharedStorage,\n",
        "                 replay_buffer: ReplayBuffer):\n",
        "  while True:\n",
        "    network = storage.latest_network()\n",
        "    game = play_game(config, network)\n",
        "    replay_buffer.save_game(game)\n",
        "\n",
        "\n",
        "# Each game is produced by starting at the initial board position, then\n",
        "# repeatedly executing a Monte Carlo Tree Search to generate moves until the end\n",
        "# of the game is reached.\n",
        "def play_game(config: MuZeroConfig, network: Network) -> Game:\n",
        "  game = config.new_game()  \n",
        "  while not game.terminal() and len(game.history) < config.max_moves:\n",
        "    # At the root of the search tree we use the representation function to\n",
        "    # obtain a hidden state given the current observation.\n",
        "    root = Node(0)\n",
        "    current_observation = game.make_image(-1)\n",
        "    expand_node(root, game.to_play(), game.legal_actions(),\n",
        "                network.initial_inference(current_observation))\n",
        "    add_exploration_noise(config, root)\n",
        "\n",
        "    # We then run a Monte Carlo Tree Search using only action sequences and the\n",
        "    # model learned by the network.\n",
        "    run_mcts(config, root, game.action_history(), network)\n",
        "    action = select_action(config, len(game.history), root, network)\n",
        "    game.apply(action)\n",
        "    game.store_search_statistics(root)\n",
        "  return game\n",
        "\n",
        "\n",
        "# Core Monte Carlo Tree Search algorithm.\n",
        "# To decide on an action, we run N simulations, always starting at the root of\n",
        "# the search tree and traversing the tree according to the UCB formula until we\n",
        "# reach a leaf node.\n",
        "def run_mcts(config: MuZeroConfig, root: Node, action_history: ActionHistory,\n",
        "             network: Network):\n",
        "  min_max_stats = MinMaxStats(config.known_bounds)\n",
        "\n",
        "  for _ in range(config.num_simulations):\n",
        "    history = action_history.clone()\n",
        "    node = root\n",
        "    search_path = [node]\n",
        "\n",
        "    while node.expanded():\n",
        "      action, node = select_child(config, node, min_max_stats)\n",
        "      history.add_action(action)\n",
        "      search_path.append(node)\n",
        "\n",
        "    # Inside the search tree we use the dynamics function to obtain the next\n",
        "    # hidden state given an action and the previous hidden state.\n",
        "    parent = search_path[-2]\n",
        "    network_output = network.recurrent_inference(parent.hidden_state,\n",
        "                                                 history.last_action())\n",
        "    expand_node(node, history.to_play(), history.action_space(), network_output)\n",
        "\n",
        "    backpropagate(search_path, network_output.value, history.to_play(),\n",
        "                  config.discount, min_max_stats)\n",
        "\n",
        "\n",
        "def select_action(config: MuZeroConfig, num_moves: int, node: Node,\n",
        "                  network: Network):\n",
        "  visit_counts = [\n",
        "      (child.visit_count, action) for action, child in node.children.items()\n",
        "  ]\n",
        "  t = config.visit_softmax_temperature_fn(\n",
        "      num_moves=num_moves, training_steps=network.training_steps())\n",
        "  _, action = softmax_sample(visit_counts, t)\n",
        "  return action\n",
        "\n",
        "\n",
        "# Select the child with the highest UCB score.\n",
        "def select_child(config: MuZeroConfig, node: Node,\n",
        "                 min_max_stats: MinMaxStats):\n",
        "  _, action, child = max(\n",
        "      (ucb_score(config, node, child, min_max_stats), action,\n",
        "       child) for action, child in node.children.items())\n",
        "  return action, child\n",
        "\n",
        "\n",
        "# The score for a node is based on its value, plus an exploration bonus based on\n",
        "# the prior.\n",
        "def ucb_score(config: MuZeroConfig, parent: Node, child: Node,\n",
        "              min_max_stats: MinMaxStats) -> float:\n",
        "  pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /\n",
        "                  config.pb_c_base) + config.pb_c_init\n",
        "  pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "  prior_score = pb_c * child.prior\n",
        "  value_score = min_max_stats.normalize(child.value())\n",
        "  return prior_score + value_score\n",
        "\n",
        "\n",
        "# We expand a node using the value, reward and policy prediction obtained from\n",
        "# the neural network.\n",
        "def expand_node(node: Node, to_play: Player, actions: List[Action],\n",
        "                network_output: NetworkOutput):\n",
        "  node.to_play = to_play\n",
        "  node.hidden_state = network_output.hidden_state\n",
        "  node.reward = network_output.reward\n",
        "  policy = {a: math.exp(network_output.policy_logits[a]) for a in actions}\n",
        "  policy_sum = sum(policy.values())\n",
        "  for action, p in policy.items():\n",
        "    node.children[action] = Node(p / policy_sum)\n",
        "\n",
        "\n",
        "# At the end of a simulation, we propagate the evaluation all the way up the\n",
        "# tree to the root.\n",
        "def backpropagate(search_path: List[Node], value: float, to_play: Player,\n",
        "                  discount: float, min_max_stats: MinMaxStats):\n",
        "  for node in search_path:\n",
        "    node.value_sum += value if node.to_play == to_play else -value\n",
        "    node.visit_count += 1\n",
        "    min_max_stats.update(node.value())\n",
        "\n",
        "    value = node.reward + discount * value\n",
        "\n",
        "\n",
        "# At the start of each search, we add dirichlet noise to the prior of the root\n",
        "# to encourage the search to explore new actions.\n",
        "def add_exploration_noise(config: MuZeroConfig, node: Node):\n",
        "  actions = list(node.children.keys())\n",
        "  noise = numpy.random.dirichlet([config.root_dirichlet_alpha] * len(actions))\n",
        "  frac = config.root_exploration_fraction\n",
        "  for a, n in zip(actions, noise):\n",
        "    node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "######### End Self-Play ##########\n",
        "##################################\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPcv4eiEYTyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "##################################\n",
        "####### Part 2: Training #########\n",
        "def train_network(config: MuZeroConfig, storage: SharedStorage,\n",
        "                  replay_buffer: ReplayBuffer):\n",
        "  if not len(storage._networks) > 0:\n",
        "    network = Network(config.action_space_size).to(device)\n",
        "    storage.save_network(0, network)\n",
        "\n",
        "  k = 0\n",
        "  while True:\n",
        "    network = storage.latest_network()\n",
        "    network.to(device)\n",
        "    learning_rate = config.lr_init * config.lr_decay_rate**(\n",
        "        network.training_steps() / config.lr_decay_steps)\n",
        "\n",
        "    optimizer = optim.SGD(network.parameters(), lr=learning_rate, weight_decay=config.lr_decay_rate,\n",
        "                          momentum=config.momentum)\n",
        "\n",
        "    while not len(replay_buffer.buffer) > 0:\n",
        "      pass\n",
        "\n",
        "    for i in range(config.training_steps):\n",
        "      if i % config.checkpoint_interval == 0:\n",
        "        storage.save_network(i, network)      \n",
        "      batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps)\n",
        "      update_weights(batch, network, optimizer)\n",
        "    storage.save_network(config.training_steps, network)\n",
        "\n",
        "    # Test against random agent    \n",
        "    if k % 50 == 0:\n",
        "      vs_random_once = vs_random(network)\n",
        "      print('network_vs_random = ', sorted(vs_random_once.items()), end='\\n')\n",
        "    k += 1\n",
        "\n",
        "def update_weights(batch, network, optimizer):    \n",
        "  network.train()    \n",
        "\n",
        "  p_loss, v_loss = 0, 0\n",
        "\n",
        "  for image, actions, targets in batch:\n",
        "    # Initial step, from the real observation.\n",
        "    value, reward, policy_logits, hidden_state = network.initial_inference(image)\n",
        "    predictions = [(1.0, value, reward, policy_logits)]\n",
        "\n",
        "    # Recurrent steps, from action and previous hidden state.\n",
        "    for action in actions:\n",
        "      value, reward, policy_logits, hidden_state = network.recurrent_inference(hidden_state, action)\n",
        "      predictions.append((1.0 / len(actions), value, reward, policy_logits))\n",
        "\n",
        "    for prediction, target in zip(predictions, targets):\n",
        "      if(len(target[2]) > 0):\n",
        "        _ , value, reward, policy_logits = prediction\n",
        "        target_value, target_reward, target_policy = target\n",
        "\n",
        "        p_loss += torch.sum(-torch.Tensor(numpy.array(target_policy)).to(device) * torch.log(policy_logits))\n",
        "        v_loss += torch.sum((torch.Tensor([target_value]).to(device) - value) ** 2)\n",
        "  \n",
        "  optimizer.zero_grad()    \n",
        "  total_loss = (p_loss + v_loss)\n",
        "  total_loss.backward()\n",
        "  optimizer.step()\n",
        "  network.steps += 1\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] *= 0.85\n",
        "  print('p_loss %f v_loss %f' % (p_loss / len(batch), v_loss / len(batch)))\n",
        "\n",
        "######### End Training ###########\n",
        "##################################\n",
        "\n",
        "################################################################################\n",
        "############################# End of pseudocode ################################\n",
        "################################################################################\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMHQkGRaYVug",
        "colab_type": "code",
        "outputId": "9c294b40-b8a8-466a-f5e3-4ebc1cc120e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(device)\n",
        "\n",
        "# Stubs to make the typechecker happy.\n",
        "def softmax_sample(distribution, temperature: float):\n",
        "  if temperature == 0:\n",
        "    temperature = 1\n",
        "  distribution = numpy.array(distribution)**(1/temperature)\n",
        "  p_sum = distribution.sum()\n",
        "  sample_temp = distribution/p_sum\n",
        "  return 0, numpy.argmax(numpy.random.multinomial(1, sample_temp, 1))\n",
        "\n",
        "def launch_job(f, *args):\n",
        "  f(*args)\n",
        "\n",
        "def make_uniform_network():\n",
        "  return Network(make_connect4_config().action_space_size).to(device)\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irYbFinAZwnR",
        "colab_type": "code",
        "outputId": "5b353b54-a479-4962-945f-4d2d17be27ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "config = make_connect4_config()\n",
        "vs_random_once = random_vs_random()\n",
        "print('random_vs_random = ', sorted(vs_random_once.items()), end='\\n')\n",
        "network = muzero(config)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random_vs_random =  [(-1, 49), (1, 51)]\n",
            "p_loss 7.805110 v_loss 2.886332\n",
            "p_loss 8.516365 v_loss 11.750000\n",
            "p_loss 8.616860 v_loss 11.250000\n",
            "p_loss 7.755169 v_loss 11.750000\n",
            "p_loss 7.774749 v_loss 13.500000\n",
            "network_vs_random =  [(-1, 29), (1, 71)]\n",
            "p_loss 7.782623 v_loss 9.000000\n",
            "p_loss 7.775730 v_loss 9.750000\n",
            "p_loss 7.774802 v_loss 6.750000\n",
            "p_loss 7.778917 v_loss 6.500000\n",
            "p_loss 7.781566 v_loss 8.000000\n",
            "p_loss 7.779751 v_loss 9.500000\n",
            "p_loss 7.557150 v_loss 8.437500\n",
            "p_loss 7.775345 v_loss 8.500000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}